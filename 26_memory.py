# 大语言模型本身是无状态的，这意味这每次向LLM发送一个请求时，它都会忘记之前的所有交互
# 这对于构建聊天机器人、代理或者需要上下文感知的应用来说，这种无状态是致命的
# 记忆(memory)是LangChain解决这个问题的核心组件。它允许LLM应用程序记住过去的交互，从而
# 在多轮对话中保持长下文和连贯性
#
# 为什么记忆重要
# 1.上下文感知：聊天机器人需要记住用户之前说过什么，才能理解后续的问题并给出相关的回答
# 2.个性化：代理可以记住用户的偏好或历史行为，从而提供更个性化的服务
# 3.避免重复：代理可以记住它已经执行过的操作或者已经获取过的信息，避免重复劳动
# 4.学习：代理可以从过去的成功或失败中学习，从而改进其未来的决策
#
# 记忆的工作原理
# LangChain的就组件通常通过管理一个chat_history变量来实现。这个变量存储了HumangMessage和
# AIMessage的列表。当链或者代理被调用时，记忆组件会将这个chat_history注入到提示中，从而为
# LLM提供上下文
#
# 常见的记忆类型
# 1.ChatMessageHistory(基础消息历史)：
# 1.1 特点：最基础的聊天消息存储方式，直接存储HumanMessage和AIMessage对象的列表，它是构建所有更复杂记忆类型的基础
# 1.2 有点：灵活、直接、易于理解和操作
# 1.3 缺点：自身不提供高级管理策略(如限制长度、总结等)，需要手动处理
